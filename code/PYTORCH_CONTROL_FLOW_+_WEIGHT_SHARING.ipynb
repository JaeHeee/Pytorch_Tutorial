{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PYTORCH: CONTROL FLOW + WEIGHT SHARING.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMKeJrI/4BlCaShxPfPvPYa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaeHeee/Pytorch_Tutorial/blob/main/code/PYTORCH_CONTROL_FLOW_%2B_WEIGHT_SHARING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrQkwcAbPeQv"
      },
      "source": [
        "## PYTORCH: CONTROL FLOW + WEIGHT SHARING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_T8foWgPmux"
      },
      "source": [
        "PyTorch 동적 그래프의 강력함을 보여주기 위해, 매우 이상한 모델을 구현.  \n",
        "\n",
        "a fully-connected ReLU network that on each forward pass randomly chooses a number between 1 and 4 and has that many hidden layers, reusing the same weights multiple times to compute the innermost hidden layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOamtvzjPdRz",
        "outputId": "0ff33c1c-ad97-45e1-a0d7-9eca754a6a63"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "\n",
        "\n",
        "class DynamicNet(torch.nn.Module):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super(DynamicNet, self).__init__()\n",
        "        self.input_linear = torch.nn.Linear(D_in, H)\n",
        "        self.middle_linear = torch.nn.Linear(H, H)\n",
        "        self.output_linear = torch.nn.Linear(H, D_out)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h_relu = self.input_linear(x).clamp(min=0)\n",
        "        for _ in range(random.randint(0, 3)):\n",
        "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
        "        y_pred = self.output_linear(h_relu)\n",
        "        return y_pred\n",
        "\n",
        "# N : batch size\n",
        "# D_in : input dimension\n",
        "# H : hidden dimension\n",
        "# D_out : output dimension\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# random Tensors to hold input and outputs\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "# Construct out model by instantiating the class defined above\n",
        "model = DynamicNet(D_in, H, D_out)\n",
        "\n",
        "# Construct our loss function and an Optimizer\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
        "for t in range(500):\n",
        "    # Forward pass\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # loss\n",
        "    loss = criterion(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # backward pass 단계 전에 optimizer 객체를 사용하여 update할 변수에 대한 gradient를 0 으로 만든다.\n",
        "    # 이렇게 하는 이유는 기본적으로 .backward()를 호출할 때마다 gradient가 버퍼(buffer)에 (덮어쓰지 않고) 누적되기 때문\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass : model의 parameter에 대한 loss의 gradient를 계산\n",
        "    loss.backward()\n",
        "\n",
        "    # Optimizer의 step 함수를 호출하면 Update the weights\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 44.13999938964844\n",
            "199 2.320537805557251\n",
            "299 0.7980716824531555\n",
            "399 0.22657804191112518\n",
            "499 0.41260507702827454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVBq4mj7SQTS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}