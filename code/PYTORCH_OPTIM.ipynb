{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PYTORCH: OPTIM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOYw6eifOb54WWRIw4eqKIY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaeHeee/Pytorch_Tutorial/blob/main/code/PYTORCH_OPTIM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZCabD0fLmuy"
      },
      "source": [
        "## PYTORCH: OPTIM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCIfd6iELs3I"
      },
      "source": [
        "optim 패키지를 사용하여 가중치를 갱신할 Optimizer를 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEXxrK8wLjzq",
        "outputId": "7fcc32fc-deef-4ae2-a2b5-49cbc679158e"
      },
      "source": [
        "import torch\n",
        "\n",
        "# N : batch size\n",
        "# D_in : input dimension\n",
        "# H : hidden dimension\n",
        "# D_out : output dimension\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# random Tensors to hold input and outputs\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H, D_out),\n",
        ")\n",
        "\n",
        "# nn package에는 loss function들도 포함되어 있다.\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-4\n",
        "# optim package를 사용하여 model의 weight를 update할 optimizer를 정의\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for t in range(500):\n",
        "    # Forward pass\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # backward pass 단계 전에 optimizer 객체를 사용하여 update할 변수에 대한 gradient를 0 으로 만든다.\n",
        "    # 이렇게 하는 이유는 기본적으로 .backward()를 호출할 때마다 gradient가 버퍼(buffer)에 (덮어쓰지 않고) 누적되기 때문\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass : model의 parameter에 대한 loss의 gradient를 계산\n",
        "    loss.backward()\n",
        "\n",
        "    # Optimizer의 step 함수를 호출하면 Update the weights\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 59.85490036010742\n",
            "199 1.525457739830017\n",
            "299 0.06200653687119484\n",
            "399 0.0019503453513607383\n",
            "499 2.423727528366726e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrCaOIjYM-my"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}