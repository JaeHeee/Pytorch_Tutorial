{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PYTORCH: TENSORS AND AUTOGRAD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONuo0n17Kaor3olJea/ojG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaeHeee/Pytorch_Tutorial/blob/main/code/PYTORCH_TENSORS_AND_AUTOGRAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEirRKs3a27y"
      },
      "source": [
        "### PYTORCH: TENSORS AND AUTOGRAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJMJg1wya2Mo"
      },
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# N : batch size\n",
        "# D_in : input dimension\n",
        "# H : hidden dimension\n",
        "# D_out : output dimension\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# random Tensors to hold input and outputs\n",
        "# requires_grad = False로 설정하여 backprop 중에 이 Tensor들에 대한 gradient를 계산할 필요가 없음을 나타낸다.\n",
        "# requires_grad의 기본값은 False\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "# random Tensors for weights\n",
        "# requires_grad = True로 설정하여 backprop 중에 이 Tensor들에 대한 gradient를 계산할 필요가 있음을 나타낸다.\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "    # Forward pass\n",
        "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
        "\n",
        "    # loss\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Use autograd to compute the backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    # torch.no_grad()로 감싸는 이유 : 가중치들이 requires_grad = True이지만 autograd에서는 이를 추적할 필요가 없다.\n",
        "    with torch.no_grad():\n",
        "        w1 -= learning_rate * grad_w1\n",
        "        w2 -= learning_rate * grad_w2\n",
        "    \n",
        "        # weight update 후에는 수동으로 gradient를 0으로 만든다.\n",
        "        w1.grad.zero_()\n",
        "        w2.grad.zero_()        \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}